{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-m80KgQ1-Z3"
      },
      "source": [
        "# CS148 Project 2 - Binary Classification Comparative Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSR9laCN1-Z5"
      },
      "source": [
        "For this project we're going to attempt a binary classification of a dataset using multiple methods and compare results. \n",
        "\n",
        "Our goals for this project will be to introduce you to several of the most common classification techniques, how to perform them and tweek parameters to optimize outcomes, how to produce and interpret results, and compare performance. You will be asked to analyze your findings and provide explanations for observed performance. \n",
        "\n",
        "Specifically you will be asked to classify whether a <b>patient is suffering from heart disease</b> based on a host of potential medical factors.\n",
        "\n",
        "<b><u>DEFINITIONS</b></u>\n",
        "\n",
        "\n",
        "<b> Binary Classification:</b>\n",
        "In this case a complex dataset has an added 'target' label with one of two options. Your learning algorithm will try to assign one of these labels to the data.\n",
        "\n",
        "<b> Supervised Learning:</b>\n",
        "This data is fully supervised, which means it's been fully labeled and we can trust the veracity of the labeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zacxg4dy1-Z7"
      },
      "source": [
        "## Background: The Dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKL6UhzY1-Z8"
      },
      "source": [
        "For this exercise we will be using a subset of the UCI Heart Disease dataset, leveraging the fourteen most commonly used attributes. All identifying information about the patient has been scrubbed. \n",
        "\n",
        "The dataset includes 14 columns. The information provided by each column is as follows:\n",
        "<ul>\n",
        "    <li><b>age:</b> Age in years</li>\n",
        "    <li><b>sex:</b> (1 = male; 0 = female)</li>\n",
        "    <li><b>cp:</b> Chest pain type (0 = asymptomatic; 1 = atypical angina; 2 = non-anginal pain; 3 = typical angina)</li>\n",
        "    <li><b>trestbps:</b> Resting blood pressure (in mm Hg on admission to the hospital)</li>\n",
        "    <li><b>cholserum:</b> Cholestoral in mg/dl</li>\n",
        "    <li><b>fbs</b> Fasting blood sugar > 120 mg/dl (1 = true; 0 = false)</li>\n",
        "    <li><b>restecg:</b> Resting electrocardiographic results (0= showing probable or definite left ventricular hypertrophy by Estes' criteria; 1 = normal; 2 = having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV))</li>\n",
        "    <li><b>thalach:</b> Maximum heart rate achieved</li>\n",
        "    <li><b>exang:</b> Exercise induced angina (1 = yes; 0 = no)</li>\n",
        "    <li><b>oldpeakST:</b> Depression induced by exercise relative to rest</li>\n",
        "    <li><b>slope:</b> The slope of the peak exercise ST segment (0 = downsloping; 1 = flat; 2 = upsloping)</li>\n",
        "    <li><b>ca:</b> Number of major vessels (0-3) colored by flourosopy</li>\n",
        "    <li><b>thal:</b> 1 = normal; 2 = fixed defect; 7 = reversable defect</li>\n",
        "    <li><b><u>Sick:</u></b> Indicates the presence of Heart disease (True = Disease; False = No disease)</li>\n",
        "</ul>\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-OS5fRR1-Z9"
      },
      "source": [
        "## Loading Essentials and Helper Functions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89pf3hlo1-Z-"
      },
      "outputs": [],
      "source": [
        "#Here are a set of libraries we imported to complete this assignment. \n",
        "#Feel free to use these or equivalent libraries for your implementation\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt # this is used for the plot the graph \n",
        "import os\n",
        "import seaborn as sns # used for plot interactive graph.\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import SVC  \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import sklearn.metrics.cluster as smc\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "from matplotlib import pyplot\n",
        "import itertools\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import random \n",
        "  \n",
        "random.seed(42) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLA8MesY1-aD"
      },
      "source": [
        "## Part 1. Load the Data and Analyze"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cPvNFAb1-aD"
      },
      "source": [
        "Let's first load our dataset so we'll be able to work with it. (correct the relative path if your notebook is in a different directory than the csv file.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeQ4T7nD1-aE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR8RcYxs1-aF"
      },
      "source": [
        "### Now that our data is loaded, let's take a closer look at the dataset we're working with. Use the head method,  the describe method, and the info method to display some of the rows so we can visualize the types of data fields we'll be working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPdUGLr21-aF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muZ1U7Xw1-aG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU8MbRlO1-aI"
      },
      "source": [
        "### Sometimes data will be stored in different formats (e.g., string, date, boolean), but many learning methods work strictly on numeric inputs. Call the info method to determine the datafield type for each column. Are there any that are problemmatic and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdEqf_n01-aJ"
      },
      "source": [
        "[Provide Answer here:] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZJswqId1-aJ"
      },
      "source": [
        "### Determine if we're dealing with any null values. If so, report on which columns? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFiFL0Qt1-aK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6WOThx71-aL"
      },
      "source": [
        "[Discuss here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hl4xIdL1-aM"
      },
      "source": [
        "### Before we begin our analysis we need to fix the field(s) that will be problematic. Specifically convert our boolean sick variable into a binary numeric target variable (values of either '0' or '1'), and then drop the original sick datafield from the dataframe. (hint: try label encoder or .astype()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDmfWcDO1-aM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDbsCOog1-aN"
      },
      "source": [
        "### Now that we have a feel for the data-types for each of the variables, plot histograms of each field and attempt to ascertain how each variable performs (is it a binary, or limited selection, or does it follow a gradient? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYlbbySv1-aO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZwpu5xZ1-aP"
      },
      "source": [
        "### We also want to make sure we are dealing with a balanced dataset. In this case, we want to confirm whether or not we have an equitable number of  sick and healthy individuals to ensure that our classifier will have a sufficiently balanced dataset to adequately classify the two. Plot a histogram specifically of the sick target, and conduct a count of the number of sick and healthy individuals and report on the results: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qo0tH8sS1-aP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvsiRGhQ1-aQ"
      },
      "source": [
        "[Include description of findings here] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQe3iBYT1-aR"
      },
      "source": [
        "### Balanced datasets are important to ensure that classifiers train adequately and don't overfit, however arbitrary balancing of a dataset might introduce its own issues. Discuss some of the problems that might arise by artificially balancing a dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRSII0PO1-aR"
      },
      "source": [
        "[Discuss prompt here] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v1_1HFC1-aS"
      },
      "source": [
        "### Now that we have our dataframe prepared let's start analyzing our data. For this next question let's look at the correlations of our variables to our target value. First, map out the correlations between the values, and then discuss the relationships you observe. Do some research on the variables to understand why they may relate to the observed corellations. Intuitively, why do you think some variables correlate more highly than others (hint: one possible approach you can use the sns heatmap function to map the corr() method)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXAfCjem1-aS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osMk-hMm1-aT"
      },
      "source": [
        "[Discuss correlations here] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc_52Cut1-aT"
      },
      "source": [
        "## Part 2. Prepare the 'Raw' Data and run a KNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ6wr3Xc1-aU"
      },
      "source": [
        "Before running our various learning methods, we need to do some additional prep to finalize our data. Specifically you'll have to cut the classification target from the data that will be used to classify, and then you'll have to divide the dataset into training and testing cohorts.\n",
        "\n",
        "Specifically, we're going to ask you to prepare 2 batches of data: 1. Will simply be the raw numeric data that hasn't gone through any additional pre-processing. The other, will be data that you pipeline using your own selected methods. We will then feed both of these datasets into a classifier to showcase just how important this step can be!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piz-mp0o1-aV"
      },
      "source": [
        "### Save the label column as a separate array and then drop it from the dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8svzGB-A1-aV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG7ri8V11-aW"
      },
      "source": [
        "### First Create your 'Raw' unprocessed training data by dividing your dataframe into training and testing cohorts, with your training cohort consisting of 80% of your total dataframe (hint: use the train_test_split() method) Output the resulting shapes of your training and testing samples to confirm that your split was successful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmjpma2F1-aX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7TWR8en1-aa"
      },
      "source": [
        "### We'll explore how not processing your data can impact model performance by using the K-Nearest Neighbor classifier. One thing to note was because KNN's rely on Euclidean distance, they are highly sensitive to the relative magnitude of different features. Let's see that in action! Implement a K-Nearest Neighbor algorithm on our raw data and report the results. For this initial implementation simply use the default settings. Refer to the [KNN Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) for details on implementation. Report on the accuracy of the resulting model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D60t7Qe31-ab"
      },
      "outputs": [],
      "source": [
        "# k-Nearest Neighbors algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeGfA2Cf1-ab"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofBgJMdH1-ac"
      },
      "source": [
        "### Now implement a pipeline of your choice. You can opt to handle categoricals however you wish, however please scale your numeric features using standard scaler. Use the fit_transform() to fit this pipeline to your training data. and then transform() to apply that pipeline to your test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xySOY-yS1-ad"
      },
      "source": [
        "### Pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgX7OEo61-af"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline my test data\n"
      ],
      "metadata": {
        "id": "WHQZzYiTJ3US"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now retrain your model and compare the accuracy metrics with the raw and pipelined data. "
      ],
      "metadata": {
        "id": "GMbUOszrJOM0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4JqymGE1-an"
      },
      "outputs": [],
      "source": [
        "\n",
        "# k-Nearest Neighbors algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCRmyi1Y1-an"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjDfPNK01-ao"
      },
      "source": [
        "[Discuss Results here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3Kx4nH91-ap"
      },
      "source": [
        "### Parameter Optimization.  The KNN Algorithm includes an n_neighbors attribute that specifies how many neighbors to use when developing the cluster. (The default value is 5, which is what your previous model used.) Lets now try n values of: 1, 2, 3, 5, 7, 9, 10, 20, and 50. Run your model for each value and report the accuracy for each. (HINT leverage python's ability to loop to run through the array and generate results without needing to manually code each iteration)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aQa6hMT1-aq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq4htLuI1-aq"
      },
      "source": [
        "## Part 3. Additional Learning Methods "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2pXCSFS1-ar"
      },
      "source": [
        "So we have a model that seems to work well. But let's see if we can do better! To do so we'll employ multiple learning methods and compare result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qs89tu81-ar"
      },
      "source": [
        "### Linear Decision Boundary Methods "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaZ_NkZM1-as"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtWdZgsY1-as"
      },
      "source": [
        "Let's now try another classifier,one that's well known for handling linear models: Logistic Regression. Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycUesx_E1-at"
      },
      "source": [
        "### Implement a Logistical Regression Classifier. Review the [Logistical Regression Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for how to implement the model. \n",
        "\n",
        "### This time in addition to accuracy report metrics for: \n",
        "1.   Accuracy\n",
        "2.   Precision\n",
        "3.   Recall\n",
        "4.   F1 Score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZtnRX5f1-au"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxc0eyNm1-a7"
      },
      "source": [
        "### Discuss what each measure is reporting, why they are different, and why are each of these measures is significant. Explore why we might choose to evaluate the performance of differing models differently based on these factors. Try to give some specific examples of scenarios in which you might value one of these measures over the others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu8lKGYL1-a7"
      },
      "source": [
        "[Provide explanation for each measure here] \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVc5lMGZ1-av"
      },
      "source": [
        "### Let's tweak a few settings. First let's set your solver to 'sag', your max_iter= 10, and set penalty = 'none' and rerun your model. Let's see how your results change!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82CwH2261-aw"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaLDf1Wn1-ax"
      },
      "source": [
        "### Did you notice that when you ran the previous model you got the following warning: \"ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\". Check the documentation and see if you can implement a fix for this problem, and again report your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGaSV3wo1-ay"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw1YFl1Y1-a0"
      },
      "source": [
        "### Explain what you changed, and why do you think that may have altered the outcome. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYiZLct01-a1"
      },
      "source": [
        "[Provide explanation here] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M59dRwK81-a2"
      },
      "source": [
        "### Rerun your logistic classifier, but modify the penalty = 'l1', solver='liblinear' and again report the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xsw3IKrs1-a2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUxx3eFy1-a3"
      },
      "source": [
        "### Explain what what the two solver approaches are, and why liblinear may have produced an improved outcome (but not always, and it's ok if your results show otherwise!). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJZIVyvS1-a4"
      },
      "source": [
        "[Provide explanation here] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C95mrPjA1-a4"
      },
      "source": [
        "### SVM (Support Vector Machine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJYSnWKf1-a5"
      },
      "source": [
        "A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimentional space this hyperplane is a line dividing a plane in two parts where in each class lay in either side."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUAfw9mF1-a5"
      },
      "source": [
        "### Implement a Support Vector Machine classifier on your pipelined data. Review the [SVM Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) for how to implement a model. For this implementation you can simply use the default settings, but set probability = True."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kT3sHrW1-a6"
      },
      "outputs": [],
      "source": [
        "# SVM\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVNvSFJt1-a6"
      },
      "source": [
        "### Report the accuracy, precision, recall, F1 Score, of your model, but in addition, plot a Confusion Matrix of your model's performance\n",
        "\n",
        "recommend using the `from sklearn.metrics import plot_confusion_matrix` library for this one!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3If-zbZG1-a6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vzc0MVWDe7w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaXoiMYd1-a8"
      },
      "source": [
        "### Plot a Receiver Operating Characteristic curve, or ROC curve, and describe what it is and what the results indicate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pCVtTnK1-a8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7NfLZoL1-a9"
      },
      "source": [
        "[Describe what an ROC Curve is and what the results mean here] The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The area under an ROC curve is a measure of the usefulness of a test in general, where a greater area means a more useful test, so the areas under ROC curves are used to compare the usefulness of tests. Here we see a relatively low area under the curve indicating a poorly performing model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx7L-bgW1-a9"
      },
      "source": [
        "### Rerun your SVM, but now modify your model parameter kernel to equal 'linear'. Again report your Accuracy, Precision, Recall, F1 scores, and Confusion matrix and plot the new ROC curve.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6KpYps71-a-"
      },
      "outputs": [],
      "source": [
        "# SVM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wlnzo0uL1-a-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgTiftKK1-a_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW_lqTMn1-bA"
      },
      "source": [
        "### Explain the what the new results you've achieved mean. Read the documentation to understand what you've changed about your model and explain why changing that input parameter might impact the results in the manner you've observed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdji_tZn1-bA"
      },
      "source": [
        "[Provide Answer here:] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uoAdwas1-bB"
      },
      "source": [
        "### Both logistic regression and linear SVM are trying to classify data points using a linear decision boundary, then what’s the difference between their ways to find this boundary?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZvy7RTF1-bC"
      },
      "source": [
        "[Provide Answer here:] "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}